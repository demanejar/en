<!DOCTYPE html><html lang="en-US" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta property="og:image" content="https://raw.githubusercontent.com/demanejar/image-collection/main/scrapy/Best-Free-Web-Crawler.jpg"> <script async src="https://www.googletagmanager.com/gtag/js?id=G-9TR0K3B846"></script> <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8714452693053438" crossorigin="anonymous"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-9TR0K3B846'); </script><meta name="generator" content="Jekyll v4.4.1" /><meta property="og:title" content="Crawl 1000 News Websites with Scrapy and MySQL" /><meta name="author" content="trannguyenhan" /><meta property="og:locale" content="en_US" /><meta name="description" content="If we write 1 spider to analyze information for each website, it will be very time-consuming, especially for news websites. There are thousands of different news websites and they’re still growing every day." /><meta property="og:description" content="If we write 1 spider to analyze information for each website, it will be very time-consuming, especially for news websites. There are thousands of different news websites and they’re still growing every day." /><link rel="canonical" href="https://demanejar.github.io/en/posts/crawl-1000-website-new-with-scrapy/" /><meta property="og:url" content="https://demanejar.github.io/en/posts/crawl-1000-website-new-with-scrapy/" /><meta property="og:site_name" content="De Manejar" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2023-03-01T20:52:00+07:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Crawl 1000 News Websites with Scrapy and MySQL" /><meta name="twitter:site" content="@DeManejar" /><meta name="twitter:creator" content="@trannguyenhan" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"trannguyenhan"},"dateModified":"2023-03-01T20:52:00+07:00","datePublished":"2023-03-01T20:52:00+07:00","description":"If we write 1 spider to analyze information for each website, it will be very time-consuming, especially for news websites. There are thousands of different news websites and they’re still growing every day.","headline":"Crawl 1000 News Websites with Scrapy and MySQL","mainEntityOfPage":{"@type":"WebPage","@id":"https://demanejar.github.io/en/posts/crawl-1000-website-new-with-scrapy/"},"url":"https://demanejar.github.io/en/posts/crawl-1000-website-new-with-scrapy/"}</script><title>Crawl 1000 News Websites with Scrapy and MySQL | De Manejar</title><link rel="shortcut icon" href="/en/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="icon" href="/en/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="apple-touch-icon" href="/en/assets/img/favicons/apple-icon.png"><link rel="apple-touch-icon" href="/en/assets/img/favicons/apple-icon-precomposed.png"><link rel="apple-touch-icon" sizes="57x57" href="/en/assets/img/favicons/apple-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/en/assets/img/favicons/apple-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/en/assets/img/favicons/apple-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/en/assets/img/favicons/apple-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/en/assets/img/favicons/apple-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/en/assets/img/favicons/apple-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/en/assets/img/favicons/apple-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/en/assets/img/favicons/apple-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/en/assets/img/favicons/apple-icon-180x180.png"><link rel="icon" type="image/png" sizes="192x192" href="/en/assets/img/favicons/android-icon-192x192.png"><link rel="icon" type="image/png" sizes="32x32" href="/en/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/en/assets/img/favicons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="16x16" href="/en/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/en/assets/img/favicons/manifest.json"><meta name='msapplication-config' content='/en/assets/img/favicons/browserconfig.xml'><meta name="msapplication-TileColor" content="#ffffff"><meta name="msapplication-TileImage" content="/en/assets/img/favicons/ms-icon-144x144.png"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"><link rel="stylesheet" href="/en/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/en/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script defer src="/en/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/en/" alt="avatar" class="mx-auto"> <img src="https://i.pinimg.com/564x/9c/32/79/9c3279b5ac960533cbcb7e833ac4d947.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/en/">De Manejar</a></div><div class="site-subtitle font-italic">Hello Bigdata !</div></div><ul class="w-100"><li class="nav-item"> <a href="/en/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/en/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/en/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/en/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/en/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/demanejar" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href="https://twitter.com/DeManejar" aria-label="twitter" class="order-4" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['demanejar','gmail.com'].join('@')" aria-label="email" class="order-5" > <i class="fas fa-envelope"></i> </a> <a href="/en/feed.xml" aria-label="rss" class="order-6" > <i class="fas fa-rss"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/en/"> Posts </a> </span> <span>Crawl 1000 News Websites with Scrapy and MySQL</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Crawl 1000 News Websites with Scrapy and MySQL</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> trannguyenhan </span> <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Wed, Mar 1, 2023, 8:52 PM +0700" prep="on" > Mar 1, 2023 <i class="unloaded">2023-03-01T20:52:00+07:00</i> </span></div><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="1697 words">9 min</span></div></div><div class="post-content"><p>If we write 1 <code class="language-plaintext highlighter-rouge">spider</code> to analyze information for each website, it will be very time-consuming, especially for news websites. There are thousands of different news websites and they’re still growing every day.</p><p>So now there’s a problem: we need to analyze the content of 1000 news websites, and our task is to schedule crawling these 1000 news websites daily. For scheduling, we can temporarily solve it with <code class="language-plaintext highlighter-rouge">crontab</code>. So what about crawling 1000 websites? We can’t write 1000 <code class="language-plaintext highlighter-rouge">spiders</code> to parse each website! So in this article, we’ll learn more about using databases to store configurations. Specifically, in this article, we’ll use MySQL.</p><blockquote><p>Many of you may wonder: what’s the purpose of analyzing content from 1000 news websites? This problem is mainly to analyze trends, keywords, hot events by day, analyze content, identify which sites often repost articles from other sites, which sites are reputable news sites to recommend to users, and many other things.</p></blockquote><h2 id="creating-project-and-defining-item">Creating Project and Defining Item</h2><p>Create a Scrapy project and a <code class="language-plaintext highlighter-rouge">news</code> spider similar to the <code class="language-plaintext highlighter-rouge">crawl-alonhadat</code> article:</p><p><a href="https://demanejar.github.io/posts/crawl-housing-data-from-alonhadat/#t%E1%BA%A1o-m%E1%BB%99t-project-scrapy">Creating Crawl Alonhadat Project</a></p><p>Define the attributes to extract from a news article. In this article, I extract 3 attributes: URL, title, content, and time:</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre><td class="rouge-code"><pre><span class="c1"># Define here the models for your scraped items
#
# See documentation in:
# https://docs.scrapy.org/en/latest/topics/items.html
</span>
<span class="kn">import</span> <span class="n">scrapy</span>


<span class="k">class</span> <span class="nc">CrawlerItem</span><span class="p">(</span><span class="n">scrapy</span><span class="p">.</span><span class="n">Item</span><span class="p">):</span>
    <span class="c1"># define the fields for your item here like:
</span>    <span class="c1"># name = scrapy.Field()
</span>    <span class="n">url</span> <span class="o">=</span> <span class="n">scrapy</span><span class="p">.</span><span class="nc">Field</span><span class="p">()</span>
    <span class="n">title</span> <span class="o">=</span> <span class="n">scrapy</span><span class="p">.</span><span class="nc">Field</span><span class="p">()</span>
    <span class="n">content</span> <span class="o">=</span> <span class="n">scrapy</span><span class="p">.</span><span class="nc">Field</span><span class="p">()</span>
    <span class="n">date</span> <span class="o">=</span> <span class="n">scrapy</span><span class="p">.</span><span class="nc">Field</span><span class="p">()</span>
</pre></table></code></div></div><p>If you want to extract more like author, images, videos, related article lists,…, you can define additional fields to extract and write corresponding code in the <code class="language-plaintext highlighter-rouge">Spider</code>.</p><h2 id="database-design">Database Design</h2><p>We take a representative website to analyze. Here I take kenh14. Go to kenh14’s homepage first:</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://raw.githubusercontent.com/demanejar/image-collection/main/scrapy/kenh14_crawl1000news.png" alt="" /></p><p>We see that news sites each have a homepage containing a list of articles. This list of articles can be a list of featured articles or a list of all articles sorted by newest, depending on each site. So extracting articles from the homepage may not be enough articles and messy because it’s a combination of many categories. So we’ll extract articles by category. Depending on the problem to solve, we can filter to extract necessary data. For example, if your problem is to analyze daily trends, crawl articles from entertainment, world, lifestyle, media categories, but you don’t need to crawl articles from legal and car categories. Website categories are mainly shown as menu bars of each website.</p><p>Our project is scheduling to crawl daily, so unlike the <a href="https://demanejar.github.io/posts/crawl-housing-data-from-alonhadat/">Crawl Alonhadat</a> project that has to next sometimes up to 4000 pages to get enough data for machine learning or deep learning models, for projects like this, if scheduling to run daily, we only need to get the newest articles of today or at most run a few more days. In this project, for each category, I only crawl articles found on the <code class="language-plaintext highlighter-rouge">first page</code> and schedule to crawl again <code class="language-plaintext highlighter-rouge">hourly</code>. Many of you are afraid of duplicate articles, but this isn’t too big a problem. We can take the article URL and HASH it to use as ID to update to the database (Postgre, Elasticsearch,…), so duplicate key articles will be updated and won’t be added as multiple records in the database. If crawling is missing articles, we need to consider reducing frequency from 1h down to 30m, 20m, or increasing the number of articles per crawl from 1 page to 2-3 pages.</p><p>I design the database for this project with 3 tables:</p><ul><li><code class="language-plaintext highlighter-rouge">websites</code> table:</ul><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://raw.githubusercontent.com/demanejar/image-collection/main/scrapy/x_news_website.png" alt="" /></p><ul><li><code class="language-plaintext highlighter-rouge">x_path_categories</code> table:</ul><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://raw.githubusercontent.com/demanejar/image-collection/main/scrapy/x_news_x_path_categories.png" alt="" /></p><ul><li><code class="language-plaintext highlighter-rouge">x_path_contents</code> table:</ul><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://raw.githubusercontent.com/demanejar/image-collection/main/scrapy/x_news_x_path_contents.png" alt="" /></p><p>For the cases I consider in this project, these are the simplest cases: listing pages of categories of each website have the same structure, and detail pages of articles of each website also have only 1 type.</p><p>For example, for the vietnamnet site, pages like vietnamnet.vn/vn/thoi-su, vietnamnet.vn/vn/kinh-doanh, vietnamnet.vn/vn/giai-tri, vietnamnet.vn/vn/the-gioi all have the same website structure, so the <code class="language-plaintext highlighter-rouge">x_path_categories</code> table only needs to contain the website ID and store the x_path of the tag wrapping the article list.</p><p>The <code class="language-plaintext highlighter-rouge">x_path_contents</code> table is quite easy to understand. When entering a detail page, it’s the x_path to extract each piece of information we need. Here I also consider a simple case where a website only has one type of article detail page.</p><h2 id="connecting-to-database">Connecting to Database</h2><p>Install the library to connect to the database for Python:</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="n">pip3</span> <span class="n">install</span> <span class="n">mysql</span><span class="o">-</span><span class="n">connector</span><span class="o">-</span><span class="n">python</span>
</pre></table></code></div></div><p>Now that we have a database, we need to write code to connect to the database for the Scrapy project.</p><p>Create a <code class="language-plaintext highlighter-rouge">constants.py</code> file to store database information:</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="n">HOST</span> <span class="o">=</span> <span class="sh">"</span><span class="s">localhost</span><span class="sh">"</span>
<span class="n">USER</span> <span class="o">=</span> <span class="sh">"</span><span class="s">root</span><span class="sh">"</span>
<span class="n">PASSWORD</span> <span class="o">=</span> <span class="sh">"</span><span class="s">mysql12345</span><span class="sh">"</span>
<span class="n">DATABASE</span> <span class="o">=</span> <span class="sh">"</span><span class="s">x_news</span><span class="sh">"</span>
<span class="n">PORT</span> <span class="o">=</span> <span class="sh">"</span><span class="s">3306</span><span class="sh">"</span>
</pre></table></code></div></div><p>The <code class="language-plaintext highlighter-rouge">connector.py</code> file will get corresponding information from the database:</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">mysql.connector</span>
<span class="kn">from</span> <span class="n">crawler.spiders</span> <span class="kn">import</span> <span class="n">constants</span>

<span class="n">db</span> <span class="o">=</span> <span class="n">mysql</span><span class="p">.</span><span class="n">connector</span><span class="p">.</span><span class="nf">connect</span><span class="p">(</span>
    <span class="n">host</span><span class="o">=</span><span class="n">constants</span><span class="p">.</span><span class="n">HOST</span><span class="p">,</span>
    <span class="n">user</span><span class="o">=</span><span class="n">constants</span><span class="p">.</span><span class="n">USER</span><span class="p">,</span>
    <span class="n">password</span><span class="o">=</span><span class="n">constants</span><span class="p">.</span><span class="n">PASSWORD</span><span class="p">,</span>
    <span class="n">database</span><span class="o">=</span><span class="n">constants</span><span class="p">.</span><span class="n">DATABASE</span><span class="p">,</span>
    <span class="n">port</span><span class="o">=</span><span class="n">constants</span><span class="p">.</span><span class="n">PORT</span>
<span class="p">)</span>
<span class="n">cursor</span> <span class="o">=</span> <span class="n">db</span><span class="p">.</span><span class="nf">cursor</span><span class="p">()</span>

<span class="c1"># get all website in database
</span><span class="k">def</span> <span class="nf">get_all_websites</span><span class="p">():</span> 
    <span class="n">cursor</span><span class="p">.</span><span class="nf">execute</span><span class="p">(</span><span class="sh">"</span><span class="s">select * from websites</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">rows</span> <span class="o">=</span> <span class="n">cursor</span><span class="p">.</span><span class="nf">fetchall</span><span class="p">()</span>

    <span class="n">result</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">rows</span><span class="p">:</span> 
        <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">row</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">result</span>

<span class="c1"># get all url categories of website
</span><span class="k">def</span> <span class="nf">get_categories</span><span class="p">(</span><span class="n">website_id</span><span class="p">):</span>
    <span class="n">cursor</span><span class="p">.</span><span class="nf">execute</span><span class="p">(</span><span class="sh">"</span><span class="s">select * from x_path_categories where website_id = </span><span class="sh">"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">website_id</span><span class="p">))</span>
    <span class="n">rows</span> <span class="o">=</span> <span class="n">cursor</span><span class="p">.</span><span class="nf">fetchall</span><span class="p">()</span>

    <span class="n">result</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">rows</span><span class="p">:</span> 
        <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
    
    <span class="k">return</span> <span class="n">result</span>

<span class="c1"># get x_path of title, content of url website
</span><span class="k">def</span> <span class="nf">get_contents</span><span class="p">(</span><span class="n">website_id</span><span class="p">):</span>
    <span class="n">cursor</span><span class="p">.</span><span class="nf">execute</span><span class="p">(</span><span class="sh">"</span><span class="s">select * from x_path_contents where website_id = </span><span class="sh">"</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">website_id</span><span class="p">))</span>
    <span class="n">rows</span> <span class="o">=</span> <span class="n">cursor</span><span class="p">.</span><span class="nf">fetchall</span><span class="p">()</span>

    <span class="n">result</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">rows</span><span class="p">:</span> 
        <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">({</span><span class="sh">"</span><span class="s">title</span><span class="sh">"</span><span class="p">:</span> <span class="n">row</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">row</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="sh">"</span><span class="s">date</span><span class="sh">"</span><span class="p">:</span> <span class="n">row</span><span class="p">[</span><span class="mi">4</span><span class="p">]})</span>

    <span class="k">return</span> <span class="n">result</span>
</pre></table></code></div></div><h2 id="writing-spider">Writing Spider</h2><p>If everyone follows this series regularly, you’ll see that when I write <code class="language-plaintext highlighter-rouge">Spider</code>, I usually use 3 functions: <code class="language-plaintext highlighter-rouge">start_requests</code> to prepare article listing links, <code class="language-plaintext highlighter-rouge">parse_links</code> to get a list of article detail links from article listing pages, and <code class="language-plaintext highlighter-rouge">parse</code> to extract necessary information from article detail pages.</p><h3 id="start_requests-function"><code class="language-plaintext highlighter-rouge">start_requests</code> Function</h3><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">start_requests</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="n">list_websites</span> <span class="o">=</span> <span class="nf">get_all_websites</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">website</span> <span class="ow">in</span> <span class="n">list_websites</span><span class="p">:</span> 
        <span class="n">domain</span> <span class="o">=</span> <span class="n">website</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># get domain of website: https://vietnamnet.vn
</span>        <span class="n">categories</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="nf">loads</span><span class="p">(</span><span class="n">website</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span> <span class="c1"># get list category of website: /thoi-su, /chinh-tri
</span>        
        <span class="k">for</span> <span class="n">category</span> <span class="ow">in</span> <span class="n">categories</span><span class="p">:</span> 
            <span class="n">link</span> <span class="o">=</span> <span class="n">domain</span> <span class="o">+</span> <span class="n">category</span>
            <span class="k">yield</span> <span class="n">scrapy</span><span class="p">.</span><span class="nc">Request</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="n">link</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">parse_links</span><span class="p">,</span> <span class="n">meta</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">website_id</span><span class="sh">"</span><span class="p">:</span> <span class="n">website</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="sh">"</span><span class="s">domain</span><span class="sh">"</span><span class="p">:</span> <span class="n">website</span><span class="p">[</span><span class="mi">1</span><span class="p">]})</span>
</pre></table></code></div></div><p>This function creates URLs for each category of websites from data in the database (data in the database, you can see more in the database design section above). These links are article listing links. For example, the first record has <code class="language-plaintext highlighter-rouge">domain = https://vietnamnet.vn</code> and <code class="language-plaintext highlighter-rouge">category = ["/vn/thoi-su/", "/vn/kinh-doanh/", "/vn/giai-tri/", "/vn/the-gioi/"]</code>, then it will go through each category one by one, combining them into complete URLs: <code class="language-plaintext highlighter-rouge">https://vietnamnet.vn/vn/thoi-su/</code>, <code class="language-plaintext highlighter-rouge">https://vietnamnet.vn/vn/kinh-doanh/</code>, <code class="language-plaintext highlighter-rouge">https://vietnamnet.vn/vn/giai-tri/</code>, <code class="language-plaintext highlighter-rouge">https://vietnamnet.vn/vn/the-gioi/</code>. These URLs are the article listing URLs of each category. Now send them to the <code class="language-plaintext highlighter-rouge">parse_link</code> function to continue working.</p><h3 id="parse_link-function"><code class="language-plaintext highlighter-rouge">parse_link</code> Function</h3><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">parse_links</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="nf">set</span><span class="p">()</span>

    <span class="n">x_path_categories</span> <span class="o">=</span> <span class="nf">get_categories</span><span class="p">(</span><span class="n">response</span><span class="p">.</span><span class="n">meta</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">website_id</span><span class="sh">"</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">x_path_category</span> <span class="ow">in</span> <span class="n">x_path_categories</span><span class="p">:</span> 
        <span class="n">x_path</span> <span class="o">=</span> <span class="n">x_path_category</span> <span class="o">+</span> <span class="sh">"</span><span class="s">//a/@href</span><span class="sh">"</span> <span class="c1"># get all tag a, after get all attribute href contain link of post in website news
</span>        <span class="n">list_href</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="nf">xpath</span><span class="p">(</span><span class="n">x_path</span><span class="p">).</span><span class="nf">extract</span><span class="p">()</span>
        
        <span class="k">for</span> <span class="n">href</span> <span class="ow">in</span> <span class="n">list_href</span><span class="p">:</span> 
            <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">href</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">50</span> <span class="ow">and</span> <span class="p">(</span><span class="sh">"</span><span class="s">http</span><span class="sh">"</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">href</span><span class="p">):</span> <span class="c1"># link have less than 50 character maybe not is link of post
</span>                <span class="n">result</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">response</span><span class="p">.</span><span class="n">meta</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">domain</span><span class="sh">"</span><span class="p">)</span> <span class="o">+</span> <span class="n">href</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">result</span><span class="p">:</span> 
        <span class="k">yield</span> <span class="n">scrapy</span><span class="p">.</span><span class="nc">Request</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="n">item</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">parse</span><span class="p">,</span> <span class="n">meta</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">website_id</span><span class="sh">"</span><span class="p">:</span> <span class="n">response</span><span class="p">.</span><span class="n">meta</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">website_id</span><span class="sh">"</span><span class="p">)})</span>
</pre></table></code></div></div><p>In the <code class="language-plaintext highlighter-rouge">start_request</code> function, we already sent the additional parameter <code class="language-plaintext highlighter-rouge">"website_id": website[0]</code> down. <code class="language-plaintext highlighter-rouge">website_id</code> helps us find the <code class="language-plaintext highlighter-rouge">x_path</code> of the element wrapping the article list through the <code class="language-plaintext highlighter-rouge">x_path_categories</code> table. Use the <code class="language-plaintext highlighter-rouge">get_categories</code> function already written in the database connection section to get the corresponding <code class="language-plaintext highlighter-rouge">x_path</code>, extract the list of URLs of each article, and call the final <code class="language-plaintext highlighter-rouge">parse</code> function.</p><p>Note that we still need to send <code class="language-plaintext highlighter-rouge">website_id</code> down to the <code class="language-plaintext highlighter-rouge">parse</code> function so we can get <code class="language-plaintext highlighter-rouge">x_path</code> information in articles defined in the database in the <code class="language-plaintext highlighter-rouge">x_path_contents</code> table.</p><h3 id="parse-function"><code class="language-plaintext highlighter-rouge">parse</code> Function</h3><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>        
    <span class="n">posts</span> <span class="o">=</span> <span class="nf">get_contents</span><span class="p">(</span><span class="n">response</span><span class="p">.</span><span class="n">meta</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">website_id</span><span class="sh">"</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">post</span> <span class="ow">in</span> <span class="n">posts</span><span class="p">:</span> 
        <span class="n">content</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="nf">xpath</span><span class="p">(</span><span class="n">post</span><span class="p">[</span><span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">]</span> <span class="o">+</span> <span class="sh">""</span><span class="p">).</span><span class="nf">extract_first</span><span class="p">()</span>
        <span class="n">title</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="nf">xpath</span><span class="p">(</span><span class="n">post</span><span class="p">[</span><span class="sh">"</span><span class="s">title</span><span class="sh">"</span><span class="p">]</span> <span class="o">+</span> <span class="sh">"</span><span class="s">//text()</span><span class="sh">"</span><span class="p">).</span><span class="nf">extract_first</span><span class="p">()</span>
        <span class="n">date</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="nf">xpath</span><span class="p">(</span><span class="n">post</span><span class="p">[</span><span class="sh">"</span><span class="s">date</span><span class="sh">"</span><span class="p">]</span> <span class="o">+</span> <span class="sh">"</span><span class="s">//text()</span><span class="sh">"</span><span class="p">).</span><span class="nf">extract_first</span><span class="p">()</span>
        
        <span class="n">content</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">normalize</span><span class="p">(</span><span class="n">content</span><span class="p">)</span>
        <span class="n">title</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">normalize</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
        <span class="n">date</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">normalize</span><span class="p">(</span><span class="n">date</span><span class="p">)</span>
        
        <span class="n">crawlerItem</span> <span class="o">=</span> <span class="nc">CrawlerItem</span><span class="p">()</span>
        <span class="n">crawlerItem</span><span class="p">[</span><span class="sh">'</span><span class="s">content</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">content</span>
        <span class="n">crawlerItem</span><span class="p">[</span><span class="sh">'</span><span class="s">title</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">title</span>
        <span class="n">crawlerItem</span><span class="p">[</span><span class="sh">'</span><span class="s">date</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">date</span>
        <span class="n">crawlerItem</span><span class="p">[</span><span class="sh">'</span><span class="s">url</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="n">url</span>

        <span class="k">yield</span> <span class="n">crawlerItem</span>
</pre></table></code></div></div><p>This function is quite clear about its purpose. For each corresponding <code class="language-plaintext highlighter-rouge">x_path</code> in the database, extract corresponding data and return <code class="language-plaintext highlighter-rouge">CrawlerItem</code>.</p><h2 id="running-and-scheduling-project">Running and Scheduling Project</h2><p>This article is in the Crawl series, so I mainly talk about crawling and storage for crawling, and less about its flow.</p><p>You can follow the complete project with data flow from <code class="language-plaintext highlighter-rouge">scheduling crawl -&gt; queue -&gt; Spark Streaming -&gt; Spark ML</code> at <a href="https://github.com/trannguyenhan/X-news">https://github.com/trannguyenhan/X-news</a> (This source code is also quite old, some things related to Kibana, ElasticSearch are also not updated to the repo, so you mainly use it for reference)</p><p>Run the project with the command:</p><div class="language-bash highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>scrapy crawl news
</pre></table></code></div></div><p>This project I’m writing a pipeline file with output data pushed to a <code class="language-plaintext highlighter-rouge">Kafka</code> queue for another side to receive and process data later. The <code class="language-plaintext highlighter-rouge">pipelines.py</code> file is written as below:</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
</pre><td class="rouge-code"><pre><span class="c1"># Define your item pipelines here
#
# Don't forget to add your item pipeline to the ITEM_PIPELINES setting
# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html
</span>
<span class="c1"># useful for handling different item types with a single interface
</span><span class="kn">from</span> <span class="n">itemadapter</span> <span class="kn">import</span> <span class="n">ItemAdapter</span>
<span class="kn">from</span> <span class="n">kafka</span> <span class="kn">import</span> <span class="n">KafkaProducer</span>
<span class="kn">import</span> <span class="n">json</span>

<span class="k">class</span> <span class="nc">CrawlerPipeline</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">open_spider</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">producer</span> <span class="o">=</span> <span class="nc">KafkaProducer</span><span class="p">(</span><span class="n">bootstrap_servers</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">localhost:9092</span><span class="sh">'</span><span class="p">],</span> \
            <span class="n">value_serializer</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">json</span><span class="p">.</span><span class="nf">dumps</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ensure_ascii</span><span class="o">=</span><span class="bp">False</span><span class="p">).</span><span class="nf">encode</span><span class="p">(</span><span class="sh">'</span><span class="s">utf-8</span><span class="sh">'</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">topic</span> <span class="o">=</span> <span class="sh">"</span><span class="s">x_news_1</span><span class="sh">"</span>

    <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="n">line</span> <span class="o">=</span> <span class="nc">ItemAdapter</span><span class="p">(</span><span class="n">item</span><span class="p">).</span><span class="nf">asdict</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">producer</span><span class="p">.</span><span class="nf">send</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">topic</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">line</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">item</span>

</pre></table></code></div></div><p>The entire Crawl project you can refer to at the GITHUB link: <a href="https://github.com/demanejar/crawler-1000news">https://github.com/demanejar/crawler-1000news</a>.</p><p>To schedule the <code class="language-plaintext highlighter-rouge">Spider</code> to run daily, you can use <code class="language-plaintext highlighter-rouge">crontab</code>. This works quite simply, just copy the run command into the file and it scans through. See more about <code class="language-plaintext highlighter-rouge">cron</code> at <a href="https://viblo.asia/p/task-schedule-trong-laravel-naQZRkOqlvx#_crontab-0">https://viblo.asia/p/task-schedule-trong-laravel-naQZRkOqlvx#_crontab-0</a>.</p><p>Since the website doesn’t have a comment section under articles, everyone can discuss and give feedback to me at this GITHUB DISCUSSION: <a href="https://github.com/orgs/demanejar/discussions/1">https://github.com/orgs/demanejar/discussions/1</a>.</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/en/categories/crawler/'>Crawler</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/en/tags/crawler/" class="post-tag no-text-decoration" >Crawler</a> <a href="/en/tags/scrapy/" class="post-tag no-text-decoration" >Scrapy</a> <a href="/en/tags/alonhadat/" class="post-tag no-text-decoration" >alonhadat</a> <a href="/en/tags/mysql/" class="post-tag no-text-decoration" >MySQL</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Crawl 1000 News Websites with Scrapy and MySQL - De Manejar&url=https://demanejar.github.io/en/posts/crawl-1000-website-new-with-scrapy/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Crawl 1000 News Websites with Scrapy and MySQL - De Manejar&u=https://demanejar.github.io/en/posts/crawl-1000-website-new-with-scrapy/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Crawl 1000 News Websites with Scrapy and MySQL - De Manejar&url=https://demanejar.github.io/en/posts/crawl-1000-website-new-with-scrapy/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/en/posts/redis-101-(part-1)/">Redis 101 (Part I)</a><li><a href="/en/posts/Kafka-In-Depth/">Kafka In Depth</a><li><a href="/en/posts/Apache-Nifi/">Understanding Apache Nifi</a><li><a href="/en/posts/Docker/">Docker Basics and Practice</a><li><a href="/en/posts/hadoop-question/">Summary of questions about Apache Hadoop</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/en/tags/bigdata/">Bigdata</a> <a class="post-tag" href="/en/tags/crawler/">Crawler</a> <a class="post-tag" href="/en/tags/scrapy/">Scrapy</a> <a class="post-tag" href="/en/tags/apache-hadoop/">Apache Hadoop</a> <a class="post-tag" href="/en/tags/hadoop/">Hadoop</a> <a class="post-tag" href="/en/tags/hdfs/">HDFS</a> <a class="post-tag" href="/en/tags/alonhadat/">alonhadat</a> <a class="post-tag" href="/en/tags/hadoop-yarn/">Hadoop Yarn</a> <a class="post-tag" href="/en/tags/selenium/">Selenium</a> <a class="post-tag" href="/en/tags/data-ingestion/">Data Ingestion</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/en/posts/add-proxy-to-scrapy-project/"><div class="card-body"> <span class="timeago small" > Feb 15, 2023 <i class="unloaded">2023-02-15T20:52:00+07:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Configuring Proxy for Scrapy Project</h3><div class="text-muted small"><p> Proxy is probably a concept that’s no longer unfamiliar to everyone. For people working on data crawling, proxy is like an inseparable companion. In this article, I will guide how to configure prox...</p></div></div></a></div><div class="card"> <a href="/en/posts/crawl-housing-data-from-alonhadat/"><div class="card-body"> <span class="timeago small" > Jan 24, 2023 <i class="unloaded">2023-01-24T20:52:00+07:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Crawl Housing Data from Alonhadat with Scrapy</h3><div class="text-muted small"><p> In this article, I will introduce in detail how to create a project with Scrapy and use it to analyze and extract housing data from the Alonhadat website. If your machine doesn’t have Scrapy yet, y...</p></div></div></a></div><div class="card"> <a href="/en/posts/scrapy-shell/"><div class="card-body"> <span class="timeago small" > Apr 1, 2023 <i class="unloaded">2023-04-01T20:52:00+07:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Introduction to Scrapy Shell</h3><div class="text-muted small"><p> Every time we write a spider, we have to write many css selector and xpath segments to analyze information, and many times we don’t know if they’re correct or not. Each time like that, we have to r...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/en/posts/add-proxy-to-scrapy-project/" class="btn btn-outline-primary" prompt="Older"><p>Configuring Proxy for Scrapy Project</p></a> <a href="/en/posts/php-scraper/" class="btn btn-outline-primary" prompt="Newer"><p>PHP Scraper</p></a></div></div></div></div><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script> <script type="text/javascript"> const imgs = document.querySelectorAll('.post-content img'); const observer = lozad(imgs); observer.observe(); </script><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2026 <a href="https://twitter.com/DeManejar">demanejar</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div></div><script type="text/javascript"> var links = document.links; for (var i = 0, linksLength = links.length; i < linksLength; i++) { if (links[i].hostname != window.location.hostname) { links[i].target = '_blank'; } } </script></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/en/tags/bigdata/">Bigdata</a> <a class="post-tag" href="/en/tags/crawler/">Crawler</a> <a class="post-tag" href="/en/tags/scrapy/">Scrapy</a> <a class="post-tag" href="/en/tags/apache-hadoop/">Apache Hadoop</a> <a class="post-tag" href="/en/tags/hadoop/">Hadoop</a> <a class="post-tag" href="/en/tags/hdfs/">HDFS</a> <a class="post-tag" href="/en/tags/alonhadat/">alonhadat</a> <a class="post-tag" href="/en/tags/hadoop-yarn/">Hadoop Yarn</a> <a class="post-tag" href="/en/tags/selenium/">Selenium</a> <a class="post-tag" href="/en/tags/data-ingestion/">Data Ingestion</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><script src="https://cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js"></script> <script> $(function() { let initTheme = "default"; if ($("html[mode=dark]").length > 0 || ($("html[mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches ) ) { initTheme = "dark"; } let mermaidConf = { theme: initTheme /* <default|dark|forest|neutral> */ }; /* Markdown converts to HTML */ $("pre").has("code.language-mermaid").each(function() { let svgCode = $(this).children().html(); $(this).addClass("unloaded"); $(this).after(`<div class=\"mermaid\">${svgCode}</div>`); }); mermaid.initialize(mermaidConf); }); </script><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/en/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://demanejar.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script>
