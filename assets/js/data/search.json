[ { "title": "Understanding K8s Overview", "url": "/en/posts/k8s/", "categories": "Blogging", "tags": "k8s", "date": "2025-04-12 20:52:00 +0700", "snippet": "Some initial research about k8s. Relationship between containerd and docker Low level runtime is currently mainly runc High level runtime is currently mainly using containerdgraph TD; A[Docker CLI]--&amp;gt;|docker run ...|B[Docker Engine]; B[Docker Daemon]; B --&amp;gt;C[Containerd];..." }, { "title": "Installing Selenium Middleware for Scrapy", "url": "/en/posts/selenium-middleware-custom-scrapy/", "categories": "Crawler", "tags": "Crawler, Selenium, Scrapy", "date": "2024-12-03 20:52:00 +0700", "snippet": "Scrapy provides the scrapy-selenium library that allows using Selenium to extract website data before returning to the spider for processing. However, in some cases, for example, I don’t want to use normal Selenium but want to use undetected-chromedriver because it helps bypass Cloudflare on webs..." }, { "title": "Some Small Tricks When Crawling Data", "url": "/en/posts/some-trick-crawler/", "categories": "Crawler", "tags": "Crawler", "date": "2024-10-01 20:52:00 +0700", "snippet": "Javascript Variables Can Contain Necessary Data When Crawling Javascript-Rendered WebsitesWhen crawling Javascript-rendered websites like in the previous article, we have to use a front-end compiler to translate this Javascript code so they render HTML before analyzing. However, many websites whe..." }, { "title": "Spline | Data Lineage Tracking And Visualization Solution", "url": "/en/posts/data-lineage-tracking-and-visualization-solution-with-spline/", "categories": "Blogging", "tags": "Data Lineage, Spark, Spline, Spark Agent", "date": "2024-03-14 20:52:00 +0700", "snippet": "Spline is an OpenSource tool that allows automatic tracking of Data Lineage and Data Pipeline Structure. Its most common use is tracking and visualizing Data Lineage for Spark.Spline OverviewSpline is an open-source and free tool for automatically tracking data lineage and pipeline structure in p..." }, { "title": "Airflow HA Guide", "url": "/en/posts/airflow-ha/", "categories": "Blogging", "tags": "HA, Airflow", "date": "2024-02-18 20:52:00 +0700", "snippet": "In this article, I will guide installing Airflow and setting up HA for it. The environment used is VirtualBox virtual machines. Create 2 virtual machines with fixed addresses, add user and grant SSH permissions from host VAGRANT_COMMAND = ARGV[0]Vagrant.configure(&quot;2&quot;) do |config| i..." }, { "title": "Crawl Bilingual News with Scrapy and Splash", "url": "/en/posts/scrapy-with-splash/", "categories": "Crawler", "tags": "Crawler, Scrapy, Splash", "date": "2023-04-15 20:52:00 +0700", "snippet": "Learn more about the concepts, advantages and disadvantages of Client Side and Server Side Rendering through the article Client-Side and Server-Side Rendering.We’ll see that some websites use client-side rendering. Their HTML code will be generated on the user’s browser side, so when crawling, ev..." }, { "title": "Introduction to Scrapy Shell", "url": "/en/posts/scrapy-shell/", "categories": "Crawler", "tags": "Crawler, Scrapy, Scrapy Shell", "date": "2023-04-01 20:52:00 +0700", "snippet": "Every time we write a spider, we have to write many css selector and xpath segments to analyze information, and many times we don’t know if they’re correct or not. Each time like that, we have to run the project and print out the information we crawled to see if it’s correct or not. Doing this is..." }, { "title": "PHP Scraper", "url": "/en/posts/php-scraper/", "categories": "Crawler", "tags": "Crawler, PHP, PHP crawler", "date": "2023-03-15 20:52:00 +0700", "snippet": "When talking about crawling, everything probably focuses on Python and frameworks built on Python like Scrapy, Beautiful Soup, or Selenium using Python,… In this article, let’s talk a bit off-topic about a language that’s not very strong in this area: PHP.There will be many times when Python won’..." }, { "title": "Crawl 1000 News Websites with Scrapy and MySQL", "url": "/en/posts/crawl-1000-website-new-with-scrapy/", "categories": "Crawler", "tags": "Crawler, Scrapy, alonhadat, MySQL", "date": "2023-03-01 20:52:00 +0700", "snippet": "If we write 1 spider to analyze information for each website, it will be very time-consuming, especially for news websites. There are thousands of different news websites and they’re still growing every day.So now there’s a problem: we need to analyze the content of 1000 news websites, and our ta..." }, { "title": "Configuring Proxy for Scrapy Project", "url": "/en/posts/add-proxy-to-scrapy-project/", "categories": "Crawler", "tags": "Crawler, Scrapy, alonhadat, proxy", "date": "2023-02-15 20:52:00 +0700", "snippet": "Proxy is probably a concept that’s no longer unfamiliar to everyone. For people working on data crawling, proxy is like an inseparable companion. In this article, I will guide how to configure proxy for a Scrapy project. The project I use as an example for this article is the crawl alonhadat proj..." }, { "title": "Crawl Housing Data from Alonhadat with Scrapy", "url": "/en/posts/crawl-housing-data-from-alonhadat/", "categories": "Crawler", "tags": "Crawler, Scrapy, alonhadat", "date": "2023-01-24 20:52:00 +0700", "snippet": "In this article, I will introduce in detail how to create a project with Scrapy and use it to analyze and extract housing data from the Alonhadat website. If your machine doesn’t have Scrapy yet, you can install it with pip. See details at https://pypi.org/project/Scrapy/. If you’re interested i..." }, { "title": "Why Website Owners Don&#39;t Completely Want to Protect Their Websites from Being Crawled?", "url": "/en/posts/no-need-protected-website-from-scraping/", "categories": "Crawler", "tags": "Crawler, Scrapy, Selenium, Protected Website", "date": "2023-01-24 20:52:00 +0700", "snippet": "Current websites are no longer as easy to extract data from as before because the structure of websites now is also very different from before. They don’t have clearly defined parts for quick analysis, and a large part is because some medium and large websites have also applied some measures to p..." }, { "title": "Crawler, Some Things I Share About Crawlers and Upcoming Crawler Series?", "url": "/en/posts/what-is-crawler-and-something/", "categories": "Crawler", "tags": "Crawler, Scrapy, Selenium", "date": "2023-01-12 20:52:00 +0700", "snippet": "Crawler, Web Scrape, Web Scraping, data collection, data scraping,… are probably the words we use most to talk about creating programs that analyze and extract data from websites. There is definitely a difference between the two concepts web crawler and web scraping, however, basically they are s..." }, { "title": "Project Socket Stream with Spark Streaming", "url": "/en/posts/socket-stream/", "categories": "Hadoop & Spark, Spark", "tags": "Spark Streaming, Bigdata, Spark", "date": "2021-09-23 20:52:00 +0700", "snippet": "In this post, we consider a small example with Spark Streaming. My work is creating a project with Spark Streaming listen in port 7777 and filter line contain “error” word and print it to console.Project preparationThis is a simple project write by Scala. You can see all project in https://github..." }, { "title": "Summary of questions about Apache Hadoop", "url": "/en/posts/hadoop-question/", "categories": "Hadoop & Spark, Hadoop", "tags": "Hadoop, Apache Hadoop, Bigdata, HDFS, Hadoop Yarn", "date": "2021-08-09 20:52:00 +0700", "snippet": "The main goal of Apache HadoopOpen data storage and powerful data processing. Save costs when storing and processing large amounts of data.You can see more details about Hadoop’s goals HEREHadoop solves the problem of fault tolerance through what technique? Hadoop is fault tolerant through r..." }, { "title": "Hadoop MapReduce and basic WordCount program with MapReduce", "url": "/en/posts/hadoop-mapreduce-and-wordcount-project/", "categories": "Hadoop & Spark, Hadoop", "tags": "Hadoop, Apache Hadoop, Bigdata, HDFS, Hadoop MapReduce, MapReduce", "date": "2021-08-03 16:00:00 +0700", "snippet": "MapReduce is a processing technique and a programming model for distributed computing to deploy and process big data. Hadoop MapReduce is a data processing framework of Hadoop built on the idea of ​​MapReduce, now when we talk about MapReduce we will immediately think of Hadoop MapReduce, so in t..." }, { "title": "Docker Basics and Practice", "url": "/en/posts/Docker/", "categories": "Blogging", "tags": "Docker", "date": "2021-07-28 20:52:00 +0700", "snippet": "This article is referenced and noted from the Docker practice series on Viblo.Link to the series: Docker Practice from BasicsI. Installing Docker and Some Basic ConceptsInstallation can be referenced in the documentationWHAT IS DOCKERBenefits of Containerized Applications Build once, use everywh..." }, { "title": "Understanding Apache Nifi", "url": "/en/posts/Apache-Nifi/", "categories": "Blogging", "tags": "Big data, Data Ingestion, Apache Nifi", "date": "2021-07-12 20:52:00 +0700", "snippet": "Apache Nifi is used to automate and control data flows between systems. It provides us with a web-based interface that can collect, process, and analyze data.NiFi is known for its ability to build automated data transfer flows between systems. Especially it supports many different types of source..." }, { "title": "Kafka In Depth", "url": "/en/posts/Kafka-In-Depth/", "categories": "Blogging", "tags": "Bigdata, Data Ingestion, Apache Kafka", "date": "2021-07-08 20:52:00 +0700", "snippet": "While working on a big data storage and processing project at school, I learned about Kafka and used it in my project. However, at that time I only knew simply that it was a message queue to pour data into, helping reading and writing from source to destination not depend on each other (loosely c..." }, { "title": "Commands for manipulating files and directories on HDFS", "url": "/en/posts/hdfs-commands/", "categories": "Hadoop & Spark, Hadoop", "tags": "Hadoop, Apache Hadoop, Bigdata, HDFS", "date": "2021-07-06 16:00:00 +0700", "snippet": "The commands on HDFS are generally quite similar to the commands on Linux, both in terms of their functions and names, if you are familiar with Linux/Ubuntu then you probably don’t need to learn much, so apply. just use it.helpAsk about common line in HDFS :hdfs dfs -helphdfs dfs -usege &amp;lt;u..." }, { "title": "HDFS", "url": "/en/posts/hdfs-introduction/", "categories": "Hadoop & Spark, Hadoop", "tags": "Hadoop, Apache Hadoop, Bigdata, HDFS", "date": "2021-07-04 16:00:00 +0700", "snippet": "Hadoop Distributed File System (HDFS) is a distributed storage system designed to run on common hardware. Highly fault-tolerant HDFS is implemented using low-cost hardware. HDFS provides high-throughput access to application data so it is well suited for applications with large data sets.Objectiv..." }, { "title": "Install and deploy Hadoop single node", "url": "/en/posts/install-and-deploy-hadoop-single-node/", "categories": "Hadoop & Spark, Hadoop", "tags": "Hadoop, Apache Hadoop, Bigdata, HDFS, Hadoop Yarn", "date": "2021-07-01 16:00:00 +0700", "snippet": "Every major industry is implementing Apache Hadoop as the standard framework for big data processing and storage. Hadoop is designed to be deployed across a network of hundreds or even thousands of dedicated servers. All these machines work together to deal with large volumes and huge data setsSe..." }, { "title": "An overview of Hadoop", "url": "/en/posts/hadoop-introduction/", "categories": "Hadoop & Spark, Hadoop", "tags": "Hadoop, Apache Hadoop, Bigdata, HDFS, Hadoop Yarn", "date": "2021-06-29 20:52:00 +0700", "snippet": "Hadoop is a framework based on a solution from Google to store and process large data. Hadoop uses the MapReduce algorithm to process input data in parallel. In short, Hadoop is used to develop applications that can perform complete statistical analysis on bulk data.See more: Mapreduce programmin..." }, { "title": "MapReduce programming model for Bigdata", "url": "/en/posts/mapreduce-programming-model/", "categories": "Hadoop & Spark, Hadoop", "tags": "Ubuntu, mapreduce, Bigdata, Java", "date": "2021-06-24 08:00:00 +0700", "snippet": "MapReduce is a processing technique and a programming model for distributed computing to deploy and process big data. MapReduce contains two important tasks: map and reduce. WordCount is a typical example of MapReduce that I will illustrate in this articleWhy was Mapreduce born?As the introductio..." }, { "title": "Redis 101 (Part I)", "url": "/en/posts/redis-101-(part-1)/", "categories": "Blogging", "tags": "NoSQL, Bigdata, Ubuntu, Redis", "date": "2021-06-10 20:52:00 +0700", "snippet": "OverviewDuring my studies in Big Data Storage and Processing at school, I learned about Redis. This is a NoSQL database. Unlike other databases, this is an in-memory storage type with key-value storage. In addition to storing data, Redis is also used for caching, message broker, and queue. Redis ..." } ]
